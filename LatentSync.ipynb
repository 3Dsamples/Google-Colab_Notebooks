{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**PREPARE ENVIRONMENT**"
      ],
      "metadata": {
        "id": "QK_OVw3gVDCZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EpLZhSJmFMBJ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "!pip install diffusers mediapipe>=0.10.8 transformers huggingface-hub omegaconf\n",
        "!pip install einops opencv-python face-alignment decord ffmpeg-python\n",
        "!pip install safetensors soundfile\n",
        "\n",
        "!git clone https://github.com/Isi-dev/LatentSync\n",
        "%cd LatentSync\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "import torch\n",
        "from omegaconf import OmegaConf\n",
        "from diffusers import AutoencoderKL, DDIMScheduler\n",
        "from latentsync.models.unet import UNet3DConditionModel\n",
        "from latentsync.pipelines.lipsync_pipeline import LipsyncPipeline\n",
        "from latentsync.whisper.audio2feature import Audio2Feature\n",
        "from diffusers.utils.import_utils import is_xformers_available\n",
        "from accelerate.utils import set_seed\n",
        "import ipywidgets as widgets\n",
        "\n",
        "os.makedirs(\"/root/.cache/torch/hub/checkpoints\", exist_ok=True)\n",
        "\n",
        "!wget https://huggingface.co/Isi99999/LatentSync/resolve/main/auxiliary/s3fd-619a316812.pth -O /root/.cache/torch/hub/checkpoints/s3fd-619a316812.pth\n",
        "!wget https://huggingface.co/Isi99999/LatentSync/resolve/main/auxiliary/2DFAN4-cd938726ad.zip -O /root/.cache/torch/hub/checkpoints/2DFAN4-cd938726ad.zip\n",
        "\n",
        "!mkdir -p checkpoints\n",
        "\n",
        "!wget https://huggingface.co/Isi99999/LatentSync/resolve/main/latentsync_unet.pt -O checkpoints/latentsync_unet.pt\n",
        "!wget https://huggingface.co/Isi99999/LatentSync/resolve/main/whisper/tiny.pt -O checkpoints/tiny.pt\n",
        "!wget https://huggingface.co/stabilityai/sd-vae-ft-mse/resolve/main/diffusion_pytorch_model.safetensors -O checkpoints/diffusion_pytorch_model.safetensors\n",
        "!wget https://huggingface.co/stabilityai/sd-vae-ft-mse/raw/main/config.json -O checkpoints/config.json\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def perform_inference(video_path, audio_path, seed=1247, num_inference_steps=20, guidance_scale=1.0, output_path=\"output_video.mp4\"):\n",
        "    config_path = \"configs/unet/first_stage.yaml\"\n",
        "    inference_ckpt_path = \"checkpoints/latentsync_unet.pt\"\n",
        "\n",
        "    config = OmegaConf.load(config_path)\n",
        "\n",
        "    is_fp16_supported = torch.cuda.is_available() and torch.cuda.get_device_capability()[0] > 7\n",
        "    dtype = torch.float16 if is_fp16_supported else torch.float32\n",
        "\n",
        "    scheduler = DDIMScheduler.from_pretrained(\"configs\")\n",
        "\n",
        "    whisper_model_path = \"checkpoints/tiny.pt\"\n",
        "    audio_encoder = Audio2Feature(model_path=whisper_model_path, device=\"cuda\", num_frames=config.data.num_frames)\n",
        "\n",
        "    vae = AutoencoderKL.from_pretrained(\"checkpoints\", torch_dtype=dtype, local_files_only=True)\n",
        "    vae.config.scaling_factor = 0.18215\n",
        "    vae.config.shift_factor = 0\n",
        "\n",
        "    unet, _ = UNet3DConditionModel.from_pretrained(\n",
        "        OmegaConf.to_container(config.model),\n",
        "        inference_ckpt_path,\n",
        "        device=\"cpu\",\n",
        "    )\n",
        "\n",
        "    unet = unet.to(dtype=dtype)\n",
        "\n",
        "    if is_xformers_available():\n",
        "        unet.enable_xformers_memory_efficient_attention()\n",
        "        print('x_formers available!')\n",
        "\n",
        "    pipeline = LipsyncPipeline(\n",
        "        vae=vae,\n",
        "        audio_encoder=audio_encoder,\n",
        "        unet=unet,\n",
        "        scheduler=scheduler,\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    set_seed(seed)\n",
        "\n",
        "    pipeline(\n",
        "        video_path=video_path,\n",
        "        audio_path=audio_path,\n",
        "        video_out_path=output_path,\n",
        "        video_mask_path=output_path.replace(\".mp4\", \"_mask.mp4\"),\n",
        "        num_frames=config.data.num_frames,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        guidance_scale=guidance_scale,\n",
        "        weight_dtype=dtype,\n",
        "        width=config.data.resolution,\n",
        "        height=config.data.resolution,\n",
        "    )\n",
        "    return output_path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RUN IMAGE TO VIDEO**"
      ],
      "metadata": {
        "id": "FQe7fJHzbOmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import cv2\n",
        "import torchaudio\n",
        "import subprocess\n",
        "\n",
        "image_upload = widgets.FileUpload(accept=\"image/*\", multiple=False, description=\"Upload Image\")\n",
        "audio_upload = widgets.FileUpload(accept=\".wav,.mp3,.aac,.flac\", multiple=False, description=\"Upload Audio\")\n",
        "seed_input = widgets.IntText(value=1247, description=\"Seed:\")\n",
        "num_steps_input = widgets.IntSlider(value=20, min=1, max=100, step=1, description=\"Steps:\")\n",
        "guidance_scale_input = widgets.FloatSlider(value=1.0, min=0.1, max=10.0, step=0.1, description=\"Guidance Scale:\")\n",
        "video_scale_input = widgets.FloatSlider(value=0.5, min=0.1, max=1.0, step=0.1, description=\"Video Scale:\")\n",
        "output_fps_input = widgets.IntSlider(value=25, min=6, max=60, step=1, description=\"Output FPS:\")\n",
        "\n",
        "run_button = widgets.Button(description=\"Run Inference\")\n",
        "output_display = widgets.Output()\n",
        "\n",
        "def convert_video_fps(input_path, target_fps):\n",
        "    if not os.path.exists(input_path) or os.path.getsize(input_path) == 0:\n",
        "        print(f\"Error: The video file {input_path} is missing or empty.\")\n",
        "        return None\n",
        "\n",
        "    output_path = f\"converted_{target_fps}fps.mp4\"\n",
        "\n",
        "    audio_check_cmd = [\n",
        "        \"ffprobe\", \"-i\", input_path, \"-show_streams\", \"-select_streams\", \"a\",\n",
        "        \"-loglevel\", \"error\"\n",
        "    ]\n",
        "    audio_present = subprocess.run(audio_check_cmd, capture_output=True, text=True).stdout.strip() != \"\"\n",
        "\n",
        "    cmd = [\n",
        "        \"ffmpeg\", \"-y\", \"-i\", input_path,\n",
        "        \"-filter:v\", f\"fps={target_fps}\",\n",
        "        \"-c:v\", \"libx264\", \"-preset\", \"fast\", \"-crf\", \"18\",\n",
        "    ]\n",
        "\n",
        "    if audio_present:\n",
        "        cmd.extend([\"-c:a\", \"aac\", \"-b:a\", \"192k\"])\n",
        "    else:\n",
        "        cmd.append(\"-an\")\n",
        "\n",
        "    cmd.append(output_path)\n",
        "\n",
        "    subprocess.run(cmd, check=True)\n",
        "    print(f\"Converted video saved as {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "# def add_silent_frames(audio_path, target_fps=25):\n",
        "\n",
        "#     waveform, sample_rate = torchaudio.load(audio_path)\n",
        "#     silent_duration = 25 / target_fps  # Two frames at target FPS\n",
        "#     silent_samples = int(silent_duration * sample_rate)\n",
        "#     silent_waveform = torch.zeros((waveform.shape[0], silent_samples))\n",
        "\n",
        "#     # Concatenate silence at the beginning for mouth correction\n",
        "#     new_waveform = torch.cat((silent_waveform, waveform), dim=1)\n",
        "#     new_audio_path = \"audio_with_silence.wav\"\n",
        "#     torchaudio.save(new_audio_path, new_waveform, sample_rate)\n",
        "\n",
        "#     return new_audio_path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def pad_audio_to_multiple_of_16(audio_path, target_fps=25):\n",
        "\n",
        "    # audio_path = add_silent_frames(audio_path)\n",
        "\n",
        "    waveform, sample_rate = torchaudio.load(audio_path)\n",
        "    audio_duration = waveform.shape[1] / sample_rate  # Duration in seconds\n",
        "\n",
        "    num_frames = int(audio_duration * target_fps)\n",
        "\n",
        "    # Pad audio to ensure frame count is a multiple of 16\n",
        "    remainder = num_frames % 16\n",
        "    if remainder > 0:\n",
        "        pad_frames = 16 - remainder\n",
        "        pad_samples = int((pad_frames / target_fps) * sample_rate)\n",
        "        pad_waveform = torch.zeros((waveform.shape[0], pad_samples))  # Silence padding\n",
        "        waveform = torch.cat((waveform, pad_waveform), dim=1)\n",
        "\n",
        "        # Save the padded audio\n",
        "        padded_audio_path = \"padded_audio.wav\"\n",
        "        torchaudio.save(padded_audio_path, waveform, sample_rate)\n",
        "    else:\n",
        "        padded_audio_path = audio_path  # No padding needed\n",
        "\n",
        "    padded_duration = waveform.shape[1] / sample_rate\n",
        "    padded_num_frames = int(padded_duration * target_fps)\n",
        "\n",
        "    return padded_audio_path, padded_num_frames\n",
        "\n",
        "\n",
        "\n",
        "def create_video_from_image(image_path, output_video_path, num_frames, fps=25):\n",
        "    \"\"\"Convert an image into a video of specified length (num_frames at 25 FPS).\"\"\"\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(\"Error: Unable to read the image.\")\n",
        "        return None\n",
        "\n",
        "    height, width, _ = img.shape\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "    for _ in range(num_frames):\n",
        "        video_writer.write(img)\n",
        "\n",
        "    video_writer.release()\n",
        "    print(f\"Created video {output_video_path} with {num_frames} frames ({num_frames / fps:.2f} seconds).\")\n",
        "    return output_video_path\n",
        "\n",
        "\n",
        "def on_run_button_click(change):\n",
        "    with output_display:\n",
        "        output_display.clear_output()\n",
        "\n",
        "        # Validate uploads\n",
        "        if not audio_upload.value or not image_upload.value:\n",
        "            print(\"Please upload both an image and an audio file.\")\n",
        "            return\n",
        "\n",
        "        # Process audio\n",
        "        audio_file_info = next(iter(audio_upload.value.values()))\n",
        "        audio_path = audio_file_info.get('name', 'uploaded_audio.wav')\n",
        "        with open(audio_path, \"wb\") as f:\n",
        "            f.write(audio_file_info['content'])\n",
        "\n",
        "        # Get audio duration with padding\n",
        "        audio_path, num_frames = pad_audio_to_multiple_of_16(audio_path, target_fps=25)\n",
        "\n",
        "        # Process image and create video\n",
        "        image_file_info = next(iter(image_upload.value.values()))\n",
        "        image_path = image_file_info.get('name', 'uploaded_image.png')\n",
        "        with open(image_path, \"wb\") as f:\n",
        "            f.write(image_file_info['content'])\n",
        "\n",
        "        img = cv2.imread(image_path)\n",
        "        if img is None:\n",
        "            print(\"Error: Could not read the image file.\")\n",
        "            return\n",
        "\n",
        "        height, width, _ = img.shape\n",
        "        video_path = \"generated_video.mp4\"\n",
        "        video_path = create_video_from_image(image_path, video_path, num_frames)\n",
        "\n",
        "        try:\n",
        "            print(\"Running inference...\")\n",
        "            output_path = \"output_video.mp4\"\n",
        "            perform_inference(video_path, audio_path, seed_input.value, num_steps_input.value, guidance_scale_input.value, output_path)\n",
        "\n",
        "            output_path = convert_video_fps(output_path, output_fps_input.value)\n",
        "\n",
        "            from IPython.display import Video\n",
        "            print(\"Inference complete. Displaying output video:\")\n",
        "            display(Video(output_path, embed=True, width=int(width * video_scale_input.value), height=int(height * video_scale_input.value)))\n",
        "\n",
        "        finally:\n",
        "            torch.cuda.empty_cache()\n",
        "            for path in [video_path, audio_path, image_path]:\n",
        "                if path and os.path.exists(path):\n",
        "                    os.remove(path)\n",
        "\n",
        "run_button.on_click(on_run_button_click)\n",
        "\n",
        "# Display the UI\n",
        "widgets_box = widgets.VBox([\n",
        "    image_upload, audio_upload,\n",
        "    seed_input, num_steps_input, guidance_scale_input, video_scale_input,\n",
        "    output_fps_input, run_button, output_display\n",
        "])\n",
        "display(widgets_box)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "t0fOMU7dZ4Qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RUN VIDEO TO VIDEO**"
      ],
      "metadata": {
        "id": "5SZG7y6OVOQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "\n",
        "import ipywidgets as widgets\n",
        "import torch\n",
        "import torchaudio\n",
        "import subprocess\n",
        "import os\n",
        "import ffmpeg\n",
        "\n",
        "\n",
        "def convert_video_fps(input_path, target_fps):\n",
        "    if not os.path.exists(input_path) or os.path.getsize(input_path) == 0:\n",
        "        print(f\"Error: The video file {input_path} is missing or empty.\")\n",
        "        return None\n",
        "\n",
        "    output_path = f\"converted_{target_fps}fps.mp4\"\n",
        "\n",
        "    audio_check_cmd = [\n",
        "        \"ffprobe\", \"-i\", input_path, \"-show_streams\", \"-select_streams\", \"a\",\n",
        "        \"-loglevel\", \"error\"\n",
        "    ]\n",
        "    audio_present = subprocess.run(audio_check_cmd, capture_output=True, text=True).stdout.strip() != \"\"\n",
        "\n",
        "    cmd = [\n",
        "        \"ffmpeg\", \"-y\", \"-i\", input_path,\n",
        "        \"-filter:v\", f\"fps={target_fps}\",\n",
        "        \"-c:v\", \"libx264\", \"-preset\", \"fast\", \"-crf\", \"18\",\n",
        "    ]\n",
        "\n",
        "    if audio_present:\n",
        "        cmd.extend([\"-c:a\", \"aac\", \"-b:a\", \"192k\"])\n",
        "    else:\n",
        "        cmd.append(\"-an\")\n",
        "\n",
        "    cmd.append(output_path)\n",
        "\n",
        "    subprocess.run(cmd, check=True)\n",
        "    print(f\"Converted video saved as {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "\n",
        "def trim_video(video_path, target_duration):\n",
        "    if not os.path.exists(video_path) or os.path.getsize(video_path) == 0:\n",
        "        print(f\"Error: The video file {video_path} is missing or empty.\")\n",
        "        return video_path\n",
        "\n",
        "    has_audio = False\n",
        "    try:\n",
        "        probe = ffmpeg.probe(video_path, v='error', select_streams='a:0', show_entries='stream=codec_type')\n",
        "        has_audio = any(stream['codec_type'] == 'audio' for stream in probe['streams'])\n",
        "    except ffmpeg.Error as e:\n",
        "        print(f\"Error while probing video: {e}\")\n",
        "        return video_path\n",
        "\n",
        "    trimmed_video_path = \"trimmed_video.mp4\"\n",
        "    try:\n",
        "        if has_audio:\n",
        "            ffmpeg.input(video_path, ss=0, to=target_duration).output(trimmed_video_path, codec=\"libx264\", audio_codec=\"aac\").run()\n",
        "        else:\n",
        "            ffmpeg.input(video_path, ss=0, to=target_duration).output(trimmed_video_path, codec=\"libx264\").run()\n",
        "        print(\"Video trimmed\")\n",
        "    except ffmpeg.Error as e:\n",
        "        print(f\"Error during video trimming: {e}\")\n",
        "        return video_path\n",
        "\n",
        "    return trimmed_video_path\n",
        "\n",
        "\n",
        "def has_audio(video_path):\n",
        "    try:\n",
        "        probe = ffmpeg.probe(video_path, v='error', select_streams='a', show_entries='stream=index')\n",
        "        return len(probe['streams']) > 0\n",
        "    except ffmpeg.Error:\n",
        "        return False\n",
        "\n",
        "def extend_video(video_path, target_duration):\n",
        "    if not os.path.exists(video_path) or os.path.getsize(video_path) == 0:\n",
        "        print(f\"Error: The video file {video_path} is missing or empty.\")\n",
        "        return video_path\n",
        "\n",
        "    audio_exists = has_audio(video_path)\n",
        "\n",
        "    try:\n",
        "        probe = ffmpeg.probe(video_path, v='error', select_streams='v:0', show_entries='format=duration')\n",
        "        original_duration = float(probe['format']['duration'])\n",
        "    except ffmpeg.Error as e:\n",
        "        print(f\"Error: Unable to fetch video duration: {e.stderr.decode()}\")\n",
        "        return video_path\n",
        "\n",
        "    if original_duration <= 0:\n",
        "        print(\"Error: Invalid video duration!\")\n",
        "        return video_path\n",
        "\n",
        "    print(\"Extending video...\")\n",
        "\n",
        "    clips = [video_path]\n",
        "    total_duration = original_duration\n",
        "    extensions = 0\n",
        "\n",
        "    while total_duration < target_duration:\n",
        "        extensions += 1\n",
        "        # reversed_clip = reverse_video(clips[-1], audio_exists)\n",
        "        clips.append(clips[-1])\n",
        "        total_duration += original_duration\n",
        "\n",
        "    print(f\"The video was extended {extensions} time(s)\")\n",
        "\n",
        "    extended_video_path = \"extended_video.mp4\"\n",
        "\n",
        "    try:\n",
        "        inputs = [ffmpeg.input(clip) for clip in clips]\n",
        "\n",
        "        if audio_exists:\n",
        "            concat = ffmpeg.concat(*inputs, v=1, a=1).output(extended_video_path, codec=\"libx264\", audio_codec=\"aac\", format=\"mp4\", vcodec=\"libx264\", acodec=\"aac\")\n",
        "        else:\n",
        "            concat = ffmpeg.concat(*inputs, v=1, a=0).output(extended_video_path, codec=\"libx264\", format=\"mp4\", vcodec=\"libx264\")\n",
        "\n",
        "        concat.run(overwrite_output=True)\n",
        "    except ffmpeg.Error as e:\n",
        "        print(f\"Error during video concatenation: {e.stderr.decode()}\")\n",
        "        return video_path\n",
        "\n",
        "    for clip in clips[1:]:\n",
        "        if os.path.exists(clip):\n",
        "            os.remove(clip)\n",
        "\n",
        "    return extended_video_path\n",
        "\n",
        "\n",
        "def reverse_video(video_path, audio_exists):\n",
        "    reversed_video_path = f\"reversed_{os.path.basename(video_path)}\"\n",
        "\n",
        "    try:\n",
        "        if audio_exists:\n",
        "            ffmpeg.input(video_path).output(reversed_video_path, vf='reverse', af='areverse').run(overwrite_output=True)\n",
        "        else:\n",
        "            ffmpeg.input(video_path).output(reversed_video_path, vf='reverse').run(overwrite_output=True)\n",
        "    except ffmpeg.Error as e:\n",
        "        print(f\"Error during video reversal: {e.stderr.decode()}\")\n",
        "        return video_path\n",
        "\n",
        "    return reversed_video_path\n",
        "\n",
        "\n",
        "def get_video_duration(video_path):\n",
        "    try:\n",
        "        probe = ffmpeg.probe(video_path, v='error', select_streams='v:0', show_entries='format=duration')\n",
        "        return float(probe['format']['duration'])\n",
        "    except ffmpeg.Error as e:\n",
        "        print(f\"Error: Unable to fetch video duration for {video_path}: {e}\")\n",
        "        return 0\n",
        "\n",
        "\n",
        "def pad_audio_to_multiple_of_16(audio_path, target_fps=25):\n",
        "    waveform, sample_rate = torchaudio.load(audio_path)\n",
        "    audio_duration = waveform.shape[1] / sample_rate\n",
        "    num_frames = int(audio_duration * target_fps)\n",
        "    remainder = num_frames % 16\n",
        "\n",
        "    if remainder > 0:\n",
        "        pad_frames = 16 - remainder\n",
        "        pad_samples = int((pad_frames / target_fps) * sample_rate)\n",
        "        pad_waveform = torch.zeros((waveform.shape[0], pad_samples))\n",
        "        waveform = torch.cat((waveform, pad_waveform), dim=1)\n",
        "        padded_audio_path = \"padded_audio.wav\"\n",
        "        torchaudio.save(padded_audio_path, waveform, sample_rate)\n",
        "    else:\n",
        "        padded_audio_path = audio_path\n",
        "\n",
        "    return padded_audio_path, int((waveform.shape[1] / sample_rate) * target_fps), waveform.shape[1] / sample_rate\n",
        "\n",
        "video_upload = widgets.FileUpload(accept=\".mp4\", multiple=False, description=\"Upload Video\")\n",
        "audio_upload = widgets.FileUpload(accept=\".wav,.mp3,.aac,.flac\", multiple=False, description=\"Upload Audio\")\n",
        "seed_input = widgets.IntText(value=1247, description=\"Seed:\")\n",
        "num_steps_input = widgets.IntSlider(value=20, min=1, max=100, step=1, description=\"Steps:\")\n",
        "guidance_scale_input = widgets.FloatSlider(value=1.0, min=0.1, max=10.0, step=0.1, description=\"Guidance Scale:\")\n",
        "video_scale_input = widgets.FloatSlider(value=0.5, min=0.1, max=1.0, step=0.1, description=\"Video Scale:\")\n",
        "output_fps_input = widgets.IntSlider(value=25, min=6, max=60, step=1, description=\"Output FPS:\")\n",
        "width, height = 0, 0\n",
        "\n",
        "run_button = widgets.Button(description=\"Run Inference\")\n",
        "output_display = widgets.Output()\n",
        "\n",
        "def on_run_button_click(change):\n",
        "    with output_display:\n",
        "        output_display.clear_output()\n",
        "\n",
        "        if not video_upload.value or not audio_upload.value:\n",
        "            print(\"Please upload both video and audio files.\")\n",
        "            return\n",
        "\n",
        "        video_file_info = next(iter(video_upload.value.values()))\n",
        "        video_path = \"uploaded_video.mp4\"\n",
        "        with open(video_path, \"wb\") as f:\n",
        "            f.write(video_file_info['content'])\n",
        "\n",
        "        global width, height\n",
        "        if width <= 0 or height <= 0:\n",
        "            print(\"Setting output video's width & height.\")\n",
        "            import cv2\n",
        "            cap = cv2.VideoCapture(video_path)\n",
        "            if cap.isOpened():\n",
        "                width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "                height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "            else:\n",
        "                print(\"Error: Unable to open video file.\")\n",
        "            cap.release()\n",
        "\n",
        "\n",
        "        audio_file_info = next(iter(audio_upload.value.values()))\n",
        "        audio_path = \"uploaded_audio.mp3\"\n",
        "        with open(audio_path, \"wb\") as f:\n",
        "            f.write(audio_file_info['content'])\n",
        "\n",
        "        video_path = convert_video_fps(video_path, 25)\n",
        "\n",
        "        audio_path, num_frames, audio_duration = pad_audio_to_multiple_of_16(audio_path, target_fps=25)\n",
        "\n",
        "\n",
        "        video_duration = get_video_duration (video_path)\n",
        "\n",
        "        if audio_duration > video_duration:\n",
        "            video_path = extend_video(video_path, audio_duration)\n",
        "            video_duration = get_video_duration (video_path)\n",
        "            if video_duration > audio_duration:\n",
        "                video_path = trim_video(video_path, audio_duration)\n",
        "\n",
        "        elif video_duration > audio_duration:\n",
        "            video_path = trim_video(video_path, audio_duration)\n",
        "\n",
        "        try:\n",
        "            print(\"Running inference...\")\n",
        "            output_path = \"output_video.mp4\"\n",
        "            perform_inference(video_path, audio_path, seed_input.value, num_steps_input.value, guidance_scale_input.value, output_path)\n",
        "\n",
        "            output_path = convert_video_fps(output_path, output_fps_input.value)\n",
        "\n",
        "            print(\"Inference complete. Displaying output video:\")\n",
        "            from IPython.display import Video\n",
        "            if width <= 0 :\n",
        "                display(Video(output_path, embed=True))\n",
        "            else:\n",
        "                display(Video(output_path, embed=True, width=int(width * video_scale_input.value), height=int(height * video_scale_input.value)))\n",
        "\n",
        "            # print(\"Download output video\")\n",
        "            # files.download(output_path)\n",
        "\n",
        "        finally:\n",
        "            torch.cuda.empty_cache()\n",
        "            for file in [video_path, audio_path]:\n",
        "                if os.path.exists(file):\n",
        "                    os.remove(file)\n",
        "\n",
        "run_button.on_click(on_run_button_click)\n",
        "widgets_box = widgets.VBox([video_upload, audio_upload, seed_input, num_steps_input, guidance_scale_input, video_scale_input, output_fps_input, run_button, output_display])\n",
        "display(widgets_box)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XLoD3kfbsfLv",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}