{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**PREPARE ENVIRONMENT**"
      ],
      "metadata": {
        "id": "VPMGP0sNHzmm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qyNaUoB-aVd",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "!apt update && apt install -y espeak-ng\n",
        "!git clone https://github.com/Isi-dev/Zonos.git\n",
        "%cd Zonos\n",
        "!pip install -e .\n",
        "!pip install --no-build-isolation -e .[compile] # optional but needed to run the hybrid"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DOWNLOAD MODELS**"
      ],
      "metadata": {
        "id": "nvB9W8c_ME76"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29n32l7X_cAb",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import torch\n",
        "import torchaudio\n",
        "from zonos.model import Zonos\n",
        "from zonos.conditioning import make_cond_dict\n",
        "\n",
        "# Check device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load model\n",
        "print(\"Loading model...\")\n",
        "model = Zonos.from_pretrained(\"Isi99999/Zonos-v0.1-transformer\", device=device)\n",
        "print(\"Model loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**UPLOAD 10 TO 30 SECONDS REFERENCE VOICE AUDIO** (optional)"
      ],
      "metadata": {
        "id": "CBzi8_NrMMeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "os.environ[\"LC_ALL\"] = \"C.UTF-8\"\n",
        "os.environ[\"LANG\"] = \"C.UTF-8\"\n",
        "os.makedirs(\"assets\", exist_ok=True)\n",
        "\n",
        "uploaded = files.upload()\n",
        "for filename in uploaded.keys():\n",
        "    new_path = \"assets/reference.mp3\"\n",
        "    if os.path.exists(new_path):\n",
        "        os.remove(new_path)\n",
        "    os.rename(filename, new_path)  # Rename safely\n",
        "\n",
        "print(\"Loading reference audio...\")\n",
        "wav, sampling_rate = torchaudio.load(\"assets/reference.mp3\")\n",
        "speaker = model.make_speaker_embedding(wav, sampling_rate)\n",
        "print(\"Reference audio loaded!\")"
      ],
      "metadata": {
        "id": "sfn6z67Zmy9s",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ENTER TEXT, ADJUST SETTINGS & RUN**"
      ],
      "metadata": {
        "id": "CjIRWivlMY8p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NOUeS9gL9fZ",
        "cellView": "form",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "text = \" I am motivated by the simple yet profound joys of being aliveâ€”the taste of a good meal, the laughter of a friend, the beauty of a sunrise, and the endless pursuit of knowledge. Even if everything about me ceases when I die, my actions, words, and ideas can leave ripples in the world, affecting others in ways I may never fully grasp. \" # @param {type:\"string\"}\n",
        "seed = 421 # @param {\"type\":\"number\"}\n",
        "use_default_speaker = True  # @param {type:\"boolean\"}\n",
        "language = 'en-us' # @param ['af', 'am', 'an', 'ar', 'as', 'az', 'ba', 'bg', 'bn', 'bpy', 'bs', 'ca', 'cmn', 'cs', 'cy', 'da', 'de', 'el', 'en-029', 'en-gb', 'en-gb-scotland', 'en-gb-x-gbclan', 'en-gb-x-gbcwmd', 'en-gb-x-rp', 'en-us', 'eo', 'es', 'es-419', 'et', 'eu', 'fa', 'fa-latn', 'fi', 'fr-be', 'fr-ch', 'fr-fr', 'ga', 'gd', 'gn', 'grc', 'gu', 'hak', 'hi', 'hr', 'ht', 'hu', 'hy', 'hyw', 'ia', 'id', 'is', 'it', 'ja', 'jbo', 'ka', 'kk', 'kl', 'kn', 'ko', 'kok', 'ku', 'ky', 'la', 'lfn', 'lt', 'lv', 'mi', 'mk', 'ml', 'mr', 'ms', 'mt', 'my', 'nb', 'nci', 'ne', 'nl', 'om', 'or', 'pa', 'pap', 'pl', 'pt', 'pt-br', 'py', 'quc', 'ro', 'ru', 'ru-lv', 'sd', 'shn', 'si', 'sk', 'sl', 'sq', 'sr', 'sv', 'sw', 'ta', 'te', 'tn', 'tr', 'tt', 'ur', 'uz', 'vi', 'vi-vn-x-central', 'vi-vn-x-south', 'yue']\n",
        "happy = 0.3077 # @param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "sad = 0.0256 # @param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "disgust = 0.0256 # @param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "fear = 0.0256 # @param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "surprise = 0.0256 # @param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "anger = 0.0256 # @param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "other = 0.2564 # @param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "neutral = 0.3077 # @param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "pitch = 20 # @param {type:\"slider\", min:0, max:400, step:1}\n",
        "speed = 15 # @param {type:\"slider\", min:0.0, max:40.0, step:1.0}\n",
        "\n",
        "\n",
        "total = happy + sad + disgust + fear + surprise + anger + other + neutral\n",
        "if total > 0:\n",
        "    happy = happy / total\n",
        "    sad = sad / total\n",
        "    disgust = disgust / total\n",
        "    fear = fear / total\n",
        "    surprise = surprise / total\n",
        "    anger = anger / total\n",
        "    other = other / total\n",
        "    neutral = neutral / total\n",
        "\n",
        "emotions = torch.tensor(list(map(float, [happy, sad, disgust, fear, surprise, anger, other, neutral])), device=device)\n",
        "\n",
        "if use_default_speaker:\n",
        "    print(\"Loading default audio...\")\n",
        "    wav, sampling_rate = torchaudio.load(\"assets/exampleaudio.mp3\")\n",
        "    speaker = model.make_speaker_embedding(wav, sampling_rate)\n",
        "    print(\"Default audio loaded!\")\n",
        "\n",
        "\n",
        "def generate_speech2( text, seed = 421, language=\"en-us\", emotion_tensor= torch.tensor(list(map(float, [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0])), device=device), pitch= 20, speed= 15):\n",
        "    \"\"\"Generate speech from text\"\"\"\n",
        "    print(f\"Generating: {text}\")\n",
        "\n",
        "    if seed >= 0:\n",
        "            torch.manual_seed(seed)\n",
        "    else:\n",
        "        torch.random.seed()\n",
        "\n",
        "    # Create conditioning\n",
        "    cond_dict = make_cond_dict(\n",
        "        text=text,\n",
        "        language=language,\n",
        "        speaker=speaker,\n",
        "        emotion=emotion_tensor,\n",
        "        pitch_std = pitch,\n",
        "        speaking_rate=speed\n",
        "\n",
        "    )\n",
        "    conditioning = model.prepare_conditioning(cond_dict)\n",
        "\n",
        "    # Generate audio\n",
        "    codes = model.generate(conditioning)\n",
        "    wavs = model.autoencoder.decode(codes).cpu()\n",
        "\n",
        "    # Save and play\n",
        "    filename = \"output.wav\"\n",
        "    torchaudio.save(filename, wavs[0], model.autoencoder.sampling_rate)\n",
        "    return filename\n",
        "\n",
        "output_file = generate_speech2(text, seed = seed, language=language, emotion_tensor= emotions, pitch = pitch, speed = speed)\n",
        "from IPython.display import Audio\n",
        "Audio(output_file)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}